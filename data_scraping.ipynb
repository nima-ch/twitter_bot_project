{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_sub_pages(base_url):\n",
    "    # Fetch the content of the main page\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the webpage\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all elements with class 'trending_item_title' and extract 'href'\n",
    "    links = soup.find_all(\"div\", class_=\"trending_item_title\")\n",
    "\n",
    "    # Extract the href attribute and create full URLs\n",
    "    sub_pages = [base_url + link.a['href'] for link in links if link.a and 'href' in link.a.attrs]\n",
    "\n",
    "    return sub_pages\n",
    "\n",
    "# URL of the main page\n",
    "base_url = \"https://www.factslides.com/\"\n",
    "\n",
    "# Get the list of sub-pages\n",
    "sub_pages = scrape_sub_pages(base_url)\n",
    "print(sub_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def scrape_description(sub_page_url):\n",
    "    response = requests.get(sub_page_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {sub_page_url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    script_tag = soup.find('script', type='application/ld+json')\n",
    "\n",
    "    if not script_tag:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON data\n",
    "        data = json.loads(script_tag.string)\n",
    "            # Parse JSON data\n",
    "        data = json.loads(script_tag.string)\n",
    "        description = data.get('articleBody', '')\n",
    "\n",
    "        # Process the text\n",
    "        doc = nlp(description)\n",
    "\n",
    "        # Extract sentences\n",
    "        facts = [sent.text.strip() for sent in doc.sents]\n",
    "        # Post-process to merge sentences that are too short\n",
    "        merged_facts = []\n",
    "        for fact in facts:\n",
    "            words = fact.split()\n",
    "            if merged_facts and len(words) <= 5:\n",
    "                merged_facts[-1] += ' ' + fact\n",
    "            else:\n",
    "                merged_facts.append(fact)\n",
    "\n",
    "        return merged_facts, description\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON from {sub_page_url}: {e}\")\n",
    "        return [] , \"\"\n",
    "\n",
    "def scrape_all_facts(sub_pages):\n",
    "    all_facts = []\n",
    "    all_desc = []\n",
    "    for sub_page in sub_pages:\n",
    "        print(f\"Scraping {sub_page}\")\n",
    "        facts, desc = scrape_description(sub_page)\n",
    "        all_facts.extend(facts)\n",
    "        all_desc.extend(desc)\n",
    "    return all_facts, \"\".join(all_desc)\n",
    "\n",
    "# Scrape all facts from sub-pages\n",
    "all_facts, all_desc = scrape_all_facts(sub_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_description_from_div(sub_page_url):\n",
    "    response = requests.get(sub_page_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {sub_page_url}\")\n",
    "        return []\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # List to store the texts of each div\n",
    "    div_texts = []\n",
    "\n",
    "    # Start with the first div\n",
    "    i = 1\n",
    "\n",
    "    # Use a while loop to find each div\n",
    "    while True:\n",
    "        div_id = f'i{i}'\n",
    "        div = soup.find('div', id=div_id)\n",
    "        if div is None:\n",
    "            # If the div is not found, break the loop\n",
    "            break\n",
    "        else:\n",
    "            # Extract text and add it to the list\n",
    "            text = div.get_text(separator=' ', strip=True)\n",
    "            div_texts.append(text)\n",
    "        i += 1  # Increment to the next div\n",
    "\n",
    "    return div_texts\n",
    "\n",
    "def scrape_all_facts(sub_pages):\n",
    "    all_facts = []\n",
    "    for sub_page in sub_pages:\n",
    "        print(f\"Scraping {sub_page}\")\n",
    "        facts_list = scrape_description_from_div(sub_page)\n",
    "        all_facts.extend(facts_list)\n",
    "    return all_facts\n",
    "\n",
    "\n",
    "fact_list_from_div = scrape_all_facts(sub_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_specific_text_from_list(strings, text_to_remove):\n",
    "    updated_strings = []\n",
    "    for string in strings:\n",
    "        if string.endswith(text_to_remove):\n",
    "            string = string[:-len(text_to_remove)]\n",
    "        updated_strings.append(string)\n",
    "    return updated_strings\n",
    "\n",
    "text_to_remove = \" ♦ SOURCE ♺ SHARE\"\n",
    "\n",
    "fact_list_from_div_trimed = remove_specific_text_from_list(fact_list_from_div, text_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "unique_fun_facts = list(set(fact_list_from_div_trimed))\n",
    "print(len(unique_fun_facts))\n",
    "\n",
    "\n",
    "shuffled_unique_fun_facts = unique_fun_facts.copy()\n",
    "random.shuffle(shuffled_unique_fun_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, fact in enumerate(shuffled_unique_fun_facts):\n",
    "    if len(fact) > 280:\n",
    "        print(f\"Line {index + 1} has more than 280 characters: {fact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fun_facts.txt', 'w') as file:\n",
    "    # Iterate over the list\n",
    "    for item in shuffled_unique_fun_facts:\n",
    "        file.write(item + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
